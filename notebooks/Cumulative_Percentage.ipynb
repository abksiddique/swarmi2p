{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfZdI_guIfHS",
        "outputId": "bd28d130-4a7e-4ef6-badc-4a7251b8f4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sniff: /content/drive/MyDrive/DataInBrief-2025/6-High-Capacity-Set.txt ---\n",
            "01: \n",
            "02: High-Capacity\t21:59:00\t\tNode\tID:\tIojVH~dnHmdHpG6ws6-MqFQ9NfIYDIPyT5pYrXH5BS8=\n",
            "03: High-Capacity\t21:59:00\t\tNode\tID:\tKQZPx5UMW8hElPMDGA1CQalD8-K~zV0XIurE5aQLnUI=\n",
            "04: High-Capacity\t21:59:00\t\tNode\tID:\tccMgCyreHWdI55Hj1ehU30cZ5G0d9YVa~6Tr9Iky7j0=\n",
            "05: High-Capacity\t21:59:00\t\tNode\tID:\ttJFdO58cakuNTpGg1uJVL2AJLatabtX8aBHnRCeIiZ4=\n",
            "\n",
            "--- Sniff: /content/drive/MyDrive/DataInBrief-2025/7-High-Capacity-Set-Freq.txt ---\n",
            "01: tXuneoZNBl-QZ3O0ceo4b8vsEGiUrplEdELmb0Ou8RU=\t906\n",
            "02: TU4AVivTZ6DfGf~npsox8OwzaQHEU8U04gZEvKwGL~I=\t686\n",
            "03: hA~tqWxfetuEhuYXq1uJ3yHKTYmcYypGLp9OTR3Z7vg=\t583\n",
            "04: N7T9mVbu6g3jb0zEbEfe0qhrh453OVClakvDLlV4h3M=\t425\n",
            "05: x-eBfsgOxUi0KreCiFKohMzh2T0nhoRz7v3oe44ROyk=\t414\n",
            "\n",
            "--- Sniff: /content/drive/MyDrive/DataInBrief-2025/8-High-Capaity-Freq2.txt ---\n",
            "01: tXuneoZNBl-QZ3O0ceo4b8vsEGiUrplEdELmb0Ou8RU=\t906\n",
            "02: TU4AVivTZ6DfGf~npsox8OwzaQHEU8U04gZEvKwGL~I=\t686\n",
            "03: hA~tqWxfetuEhuYXq1uJ3yHKTYmcYypGLp9OTR3Z7vg=\t583\n",
            "04: N7T9mVbu6g3jb0zEbEfe0qhrh453OVClakvDLlV4h3M=\t425\n",
            "05: x-eBfsgOxUi0KreCiFKohMzh2T0nhoRz7v3oe44ROyk=\t414\n",
            "[6] Extracted IDs: 629934 | Unique: 2132\n",
            "[freq] 7-High-Capacity-Set-Freq.txt -> rows=1813 | unique IDs=1364 | top=906 | total=22317\n",
            "[freq] 8-High-Capaity-Freq2.txt -> rows=1813 | unique IDs=1364 | top=906 | total=22317\n",
            "[7] Filtered to High-Capacity membership: 1003 rows (from 1813)\n",
            "[8] Filtered to High-Capacity membership: 1003 rows (from 1813)\n",
            "\n",
            "[7-High-Capacity-Set-Freq] Top-20 table (sorted by Frequency desc):\n",
            "Node ID (8 chars)  Frequency  % of Total  Cumulative %\n",
            "         tXuneoZN        906        6.99          6.99\n",
            "         TU4AVivT        686        5.29         12.28\n",
            "         x-eBfsgO        414        3.19         15.48\n",
            "         aTdMtnxQ        394        3.04         18.52\n",
            "         ZDz09qEx        270        2.08         20.60\n",
            "         g5xHgSjt        221        1.70         22.30\n",
            "         IiN-TYoM        217        1.67         23.98\n",
            "         CpMHp1xj        176        1.36         25.34\n",
            "         3UavGAjy        172        1.33         26.66\n",
            "         ~Go801IA        168        1.30         27.96\n",
            "         ZB~nvHJj        148        1.14         29.10\n",
            "         SLIdpSD6        144        1.11         30.21\n",
            "         F9577L-s        140        1.08         31.29\n",
            "         bUJ0DwK8        138        1.06         32.36\n",
            "         SC7~pNJt        137        1.06         33.41\n",
            "         z9DJ6Fjl        133        1.03         34.44\n",
            "         RaGUK5Aq        132        1.02         35.46\n",
            "         VoelZCZQ        127        0.98         36.44\n",
            "         9B8cB2oO        125        0.96         37.40\n",
            "         Lid~-zEu        114        0.88         38.28\n",
            "\n",
            "[7-High-Capacity-Set-Freq] Top-20 cumulative share of total selections: 38.28%\n",
            "[7-High-Capacity-Set-Freq] Saved: /content/drive/MyDrive/DataInBrief-2025/outputs/table2_top20_7-High-Capacity-Set-Freq.csv\n",
            "\n",
            "[8-High-Capaity-Freq2] Top-20 table (sorted by Frequency desc):\n",
            "Node ID (8 chars)  Frequency  % of Total  Cumulative %\n",
            "         tXuneoZN        906        6.99          6.99\n",
            "         TU4AVivT        686        5.29         12.28\n",
            "         x-eBfsgO        414        3.19         15.48\n",
            "         aTdMtnxQ        394        3.04         18.52\n",
            "         ZDz09qEx        270        2.08         20.60\n",
            "         g5xHgSjt        221        1.70         22.30\n",
            "         IiN-TYoM        217        1.67         23.98\n",
            "         CpMHp1xj        176        1.36         25.34\n",
            "         3UavGAjy        172        1.33         26.66\n",
            "         ~Go801IA        168        1.30         27.96\n",
            "         ZB~nvHJj        148        1.14         29.10\n",
            "         SLIdpSD6        144        1.11         30.21\n",
            "         F9577L-s        140        1.08         31.29\n",
            "         bUJ0DwK8        138        1.06         32.36\n",
            "         SC7~pNJt        137        1.06         33.41\n",
            "         z9DJ6Fjl        133        1.03         34.44\n",
            "         RaGUK5Aq        132        1.02         35.46\n",
            "         VoelZCZQ        127        0.98         36.44\n",
            "         9B8cB2oO        125        0.96         37.40\n",
            "         Lid~-zEu        114        0.88         38.28\n",
            "\n",
            "[8-High-Capaity-Freq2] Top-20 cumulative share of total selections: 38.28%\n",
            "[8-High-Capaity-Freq2] Saved: /content/drive/MyDrive/DataInBrief-2025/outputs/table2_top20_8-High-Capaity-Freq2.csv\n"
          ]
        }
      ],
      "source": [
        "# === Inspect 6 / 7 / 8 and build Top-20 tables with % and Cumulative % (fixed) ===\n",
        "import pandas as pd, re, os\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/DataInBrief-2025\"\n",
        "PATH_6 = f\"{BASE}/6-High-Capacity-Set.txt\"\n",
        "PATH_7 = f\"{BASE}/7-High-Capacity-Set-Freq.txt\"\n",
        "PATH_8 = f\"{BASE}/8-High-Capaity-Freq2.txt\"   # keep spelling as in your Drive\n",
        "\n",
        "OUTDIR = Path(BASE) / \"outputs\"\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RID_RE = re.compile(r\"([A-Za-z0-9\\-\\~\\+\\/]{43}=|[A-Za-z0-9\\-\\~\\+\\/]{44})\")\n",
        "\n",
        "def sniff(path, n=5):\n",
        "    print(f\"\\n--- Sniff: {path} ---\")\n",
        "    if not os.path.exists(path):\n",
        "        print(\"  (missing)\")\n",
        "        return\n",
        "    with open(path, \"r\", errors=\"ignore\") as f:\n",
        "        for i, ln in enumerate(f):\n",
        "            if i >= n: break\n",
        "            print(f\"{i+1:02d}: {ln.rstrip()}\")\n",
        "\n",
        "def load_hicap_set(path):\n",
        "    \"\"\"Parse 6-High-Capacity-Set.txt to extract router IDs.\"\"\"\n",
        "    ids = []\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"[6] missing: {path}\")\n",
        "        return pd.DataFrame(columns=[\"node_id\"])\n",
        "    with open(path, \"r\", errors=\"ignore\") as f:\n",
        "        for ln in f:\n",
        "            m = RID_RE.search(ln)\n",
        "            if m: ids.append(m.group(0))\n",
        "    sr = pd.Series(ids, name=\"node_id\").dropna().astype(str).str.strip()\n",
        "    uniq = sr.drop_duplicates()\n",
        "    print(f\"[6] Extracted IDs: {len(sr)} | Unique: {len(uniq)}\")\n",
        "    return uniq.to_frame()\n",
        "\n",
        "def load_freq(path):\n",
        "    \"\"\"Read whitespace-delimited (node_id, frequency).\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"[freq] missing: {path}\")\n",
        "        return pd.DataFrame(columns=[\"node_id\",\"frequency\"])\n",
        "    df = pd.read_csv(path, sep=r\"\\s+\", header=None, names=[\"node_id\",\"frequency\"], engine=\"python\")\n",
        "    df[\"frequency\"] = pd.to_numeric(df[\"frequency\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"node_id\",\"frequency\"]).copy()\n",
        "    df[\"node_id\"] = df[\"node_id\"].astype(str).str.strip()\n",
        "    df[\"frequency\"] = df[\"frequency\"].astype(int)\n",
        "    df = df.sort_values(\"frequency\", ascending=False, kind=\"mergesort\").reset_index(drop=True)\n",
        "    print(f\"[freq] {os.path.basename(path)} -> rows={len(df)} | unique IDs={df['node_id'].nunique()} | top={df['frequency'].iloc[0] if len(df) else 'NA'} | total={df['frequency'].sum()}\")\n",
        "    return df\n",
        "\n",
        "def maybe_filter_to_hc(df_freq, hc_ids, tag):\n",
        "    \"\"\"Filter to IDs present in 6-High-Capacity-Set, with a safe fallback.\"\"\"\n",
        "    if df_freq.empty or not hc_ids:\n",
        "        return df_freq\n",
        "    filtered = df_freq[df_freq[\"node_id\"].isin(hc_ids)].reset_index(drop=True)\n",
        "    if filtered.empty:\n",
        "        print(f\"[{tag}] Warning: no overlap with set 6; leaving unfiltered.\")\n",
        "        return df_freq\n",
        "    if len(filtered) < len(df_freq):\n",
        "        print(f\"[{tag}] Filtered to High-Capacity membership: {len(filtered)} rows (from {len(df_freq)})\")\n",
        "    return filtered\n",
        "\n",
        "def make_table2(df_freq, top_k=20, tag=\"file\"):\n",
        "    \"\"\"Compute % of total and cumulative %, print a Word-ready top_k table and save CSV.\"\"\"\n",
        "    if df_freq.empty:\n",
        "        print(f\"[{tag}] No data.\")\n",
        "        return None\n",
        "    total = df_freq[\"frequency\"].sum()\n",
        "    df = df_freq.copy()\n",
        "    df[\"% of Total\"] = 100.0 * df[\"frequency\"] / total\n",
        "    df[\"Cumulative %\"] = df[\"% of Total\"].cumsum()\n",
        "    top = df.head(top_k).copy()\n",
        "    top.insert(0, \"Node ID (8 chars)\", top[\"node_id\"].str[:8])\n",
        "    top = top.rename(columns={\"frequency\":\"Frequency\"})\n",
        "    top[[\"% of Total\",\"Cumulative %\"]] = top[[\"% of Total\",\"Cumulative %\"]].round(2)\n",
        "\n",
        "    print(f\"\\n[{tag}] Top-{top_k} table (sorted by Frequency desc):\")\n",
        "    print(top[[\"Node ID (8 chars)\",\"Frequency\",\"% of Total\",\"Cumulative %\"]].to_string(index=False))\n",
        "\n",
        "    top_k_share = float(top[\"Cumulative %\"].iloc[-1])\n",
        "    print(f\"\\n[{tag}] Top-{top_k} cumulative share of total selections: {top_k_share:.2f}%\")\n",
        "\n",
        "    # Save archive CSV with full node_id\n",
        "    out_csv = OUTDIR / f\"table2_top{top_k}_{tag}.csv\"\n",
        "    top[[\"node_id\",\"Frequency\",\"% of Total\",\"Cumulative %\"]].to_csv(out_csv, index=False)\n",
        "    print(f\"[{tag}] Saved: {out_csv}\")\n",
        "    return top\n",
        "\n",
        "# --- Run ---\n",
        "sniff(PATH_6, n=5)\n",
        "sniff(PATH_7, n=5)\n",
        "sniff(PATH_8, n=5)\n",
        "\n",
        "df6 = load_hicap_set(PATH_6)\n",
        "df7 = load_freq(PATH_7)\n",
        "df8 = load_freq(PATH_8)\n",
        "\n",
        "hc_ids = set(df6[\"node_id\"]) if not df6.empty else set()\n",
        "df7 = maybe_filter_to_hc(df7, hc_ids, tag=\"7\")\n",
        "df8 = maybe_filter_to_hc(df8, hc_ids, tag=\"8\")\n",
        "\n",
        "t7 = make_table2(df7, top_k=20, tag=\"7-High-Capacity-Set-Freq\")\n",
        "t8 = make_table2(df8, top_k=20, tag=\"8-High-Capaity-Freq2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2JPxwwNkIxBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE - Next**"
      ],
      "metadata": {
        "id": "NSr7d4s9LR-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6 ONLY: Parse High-Capacity membership list from \"6-High-Capacity-Set.txt\"\n",
        "import re, os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/DataInBrief-2025\"\n",
        "PATH_6 = f\"{BASE}/6-High-Capacity-Set.txt\"\n",
        "OUTDIR = Path(BASE) / \"outputs\"\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Router ID pattern: 44 chars (base64-like, with -, ~, +, /) and optional '=' padding at end.\n",
        "RID_RE = re.compile(r\"([A-Za-z0-9\\-\\~\\+\\/]{43}=|[A-Za-z0-9\\-\\~\\+\\/]{44})\")\n",
        "\n",
        "def sniff(path, n=5):\n",
        "    print(f\"\\n--- Sniff: {path} ---\")\n",
        "    if not os.path.exists(path):\n",
        "        print(\"  (missing)\"); return\n",
        "    with open(path, \"r\", errors=\"ignore\") as f:\n",
        "        for i, ln in enumerate(f):\n",
        "            if i >= n: break\n",
        "            print(f\"{i+1:02d}: {ln.rstrip()}\")\n",
        "\n",
        "# 1) Quick sniff\n",
        "sniff(PATH_6, n=5)\n",
        "\n",
        "# 2) Extract router IDs from file 6\n",
        "ids = []\n",
        "lines_total = 0\n",
        "with open(PATH_6, \"r\", errors=\"ignore\") as f:\n",
        "    for ln in f:\n",
        "        lines_total += 1\n",
        "        m = RID_RE.search(ln)\n",
        "        if m:\n",
        "            ids.append(m.group(0).strip())\n",
        "\n",
        "sr_ids = pd.Series(ids, name=\"node_id\")\n",
        "uniq_ids = sr_ids.dropna().drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "print(f\"\\n[6] lines read: {lines_total}\")\n",
        "print(f\"[6] extracted IDs: {len(sr_ids)}\")\n",
        "print(f\"[6] unique node_ids: {len(uniq_ids)}\")\n",
        "\n",
        "# 3) (Diagnostic) counts of appearances within file 6 itself\n",
        "#    This is NOT the selection frequency used in Table 2; it's just how many times an ID appears in this text file.\n",
        "counts = (sr_ids.value_counts()\n",
        "          .rename_axis(\"node_id\")\n",
        "          .reset_index(name=\"appearances_in_file6\"))\n",
        "\n",
        "print(\"\\n[6] Top 10 appearance counts in file 6 (diagnostic):\")\n",
        "print(counts.head(10).to_string(index=False))\n",
        "\n",
        "# 4) Save outputs for inspection\n",
        "unique_csv = OUTDIR / \"hc_set_unique_ids_from_file6.csv\"\n",
        "counts_csv = OUTDIR / \"hc_set_file6_appearance_counts.csv\"\n",
        "uniq_ids.to_frame().to_csv(unique_csv, index=False)\n",
        "counts.to_csv(counts_csv, index=False)\n",
        "\n",
        "print(f\"\\n[Saved] Unique IDs: {unique_csv}\")\n",
        "print(f\"[Saved] Diagnostic counts: {counts_csv}\")\n",
        "\n",
        "# 5) Show a small sample so you can check formatting\n",
        "print(\"\\nSample unique IDs:\")\n",
        "print(uniq_ids.head(5).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlJ66d32LVd4",
        "outputId": "658f5f6b-1aee-4b2f-b447-8f85be041573"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sniff: /content/drive/MyDrive/DataInBrief-2025/6-High-Capacity-Set.txt ---\n",
            "01: \n",
            "02: High-Capacity\t21:59:00\t\tNode\tID:\tIojVH~dnHmdHpG6ws6-MqFQ9NfIYDIPyT5pYrXH5BS8=\n",
            "03: High-Capacity\t21:59:00\t\tNode\tID:\tKQZPx5UMW8hElPMDGA1CQalD8-K~zV0XIurE5aQLnUI=\n",
            "04: High-Capacity\t21:59:00\t\tNode\tID:\tccMgCyreHWdI55Hj1ehU30cZ5G0d9YVa~6Tr9Iky7j0=\n",
            "05: High-Capacity\t21:59:00\t\tNode\tID:\ttJFdO58cakuNTpGg1uJVL2AJLatabtX8aBHnRCeIiZ4=\n",
            "\n",
            "[6] lines read: 629962\n",
            "[6] extracted IDs: 629934\n",
            "[6] unique node_ids: 2132\n",
            "\n",
            "[6] Top 10 appearance counts in file 6 (diagnostic):\n",
            "                                     node_id  appearances_in_file6\n",
            "Z1bh-f140eBV-XG4GVbx7zlB0xCCOLa~3AsBk9Vw~Dc=                  3926\n",
            "srLHIzJ8mfEd90JEsaWUDed2OTpaXUWTkbV4f64rXzI=                  3799\n",
            "moCfkmqXPxMoXWtIWMndXifKaTS5j-AK-g3NeI5J1HA=                  3576\n",
            "TDQO~djJ4hRVb26rrKpTzqNnGmlyZyvbEKxxEFfY8UQ=                  3369\n",
            "IojVH~dnHmdHpG6ws6-MqFQ9NfIYDIPyT5pYrXH5BS8=                  3186\n",
            "onjR2Et2hLSvokqE7b7LtThEgD~GhKo6q0raO9zoITI=                  2838\n",
            "tlVPuTPTCOboq0vpo5vPn7cG8jAnoKfgJNARbdoItbo=                  2760\n",
            "KM68Y3Pl829dEDwtl0Xw-edPa0VgBdGAZXeqmb8kfAo=                  2753\n",
            "opSZwy-wXAW4l1wIjt6~QaVvgJue47WofgvyL4h2fZo=                  2751\n",
            "Zf30eTRPt3JLPmu4a2C12cM5zgBZfODuRWYNiu6uNvg=                  2740\n",
            "\n",
            "[Saved] Unique IDs: /content/drive/MyDrive/DataInBrief-2025/outputs/hc_set_unique_ids_from_file6.csv\n",
            "[Saved] Diagnostic counts: /content/drive/MyDrive/DataInBrief-2025/outputs/hc_set_file6_appearance_counts.csv\n",
            "\n",
            "Sample unique IDs:\n",
            "IojVH~dnHmdHpG6ws6-MqFQ9NfIYDIPyT5pYrXH5BS8=\n",
            "KQZPx5UMW8hElPMDGA1CQalD8-K~zV0XIurE5aQLnUI=\n",
            "ccMgCyreHWdI55Hj1ehU30cZ5G0d9YVa~6Tr9Iky7j0=\n",
            "tJFdO58cakuNTpGg1uJVL2AJLatabtX8aBHnRCeIiZ4=\n",
            "wabNC0LPCKNWLLoctJ4TKkGoeCKuy9LHgkso66a9zWI=\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP: File 6 only â€” Top-20 IDs by appearance within file 6 (diagnostic)\n",
        "import re, os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/DataInBrief-2025\"\n",
        "PATH_6 = f\"{BASE}/6-High-Capacity-Set.txt\"\n",
        "OUTDIR = Path(BASE) / \"outputs\"\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Router ID pattern (44 chars base64-like incl -,~, +, /), optional '=' padding\n",
        "RID_RE = re.compile(r\"([A-Za-z0-9\\-\\~\\+\\/]{43}=|[A-Za-z0-9\\-\\~\\+\\/]{44})\")\n",
        "\n",
        "def sniff(path, n=5):\n",
        "    print(f\"\\n--- Sniff: {path} ---\")\n",
        "    if not os.path.exists(path):\n",
        "        print(\"  (missing)\"); return\n",
        "    with open(path, \"r\", errors=\"ignore\") as f:\n",
        "        for i, ln in enumerate(f):\n",
        "            if i >= n: break\n",
        "            print(f\"{i+1:02d}: {ln.rstrip()}\")\n",
        "\n",
        "# 1) Quick sniff\n",
        "sniff(PATH_6, n=5)\n",
        "\n",
        "# 2) Extract IDs\n",
        "ids = []\n",
        "lines_total = 0\n",
        "with open(PATH_6, \"r\", errors=\"ignore\") as f:\n",
        "    for ln in f:\n",
        "        lines_total += 1\n",
        "        m = RID_RE.search(ln)\n",
        "        if m:\n",
        "            ids.append(m.group(0).strip())\n",
        "\n",
        "sr_ids = pd.Series(ids, name=\"node_id\")\n",
        "uniq_ids = sr_ids.dropna().drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "print(f\"\\n[6] lines read: {lines_total}\")\n",
        "print(f\"[6] extracted IDs: {len(sr_ids)}\")\n",
        "print(f\"[6] unique node_ids: {len(uniq_ids)}\")\n",
        "\n",
        "# 3) Diagnostic appearance counts within file 6\n",
        "counts = (sr_ids.value_counts()\n",
        "          .rename_axis(\"node_id\")\n",
        "          .reset_index(name=\"appearances_in_file6\"))\n",
        "\n",
        "# Compute % and cumulative % within file 6 (diagnostic)\n",
        "total_appearances = counts[\"appearances_in_file6\"].sum()\n",
        "counts[\"% of File6\"] = 100.0 * counts[\"appearances_in_file6\"] / total_appearances\n",
        "counts[\"Cumulative %\"] = counts[\"% of File6\"].cumsum()\n",
        "counts[[\"% of File6\",\"Cumulative %\"]] = counts[[\"% of File6\",\"Cumulative %\"]].round(2)\n",
        "\n",
        "# Top-20 table\n",
        "top20 = counts.head(20).copy()\n",
        "top20.insert(0, \"Node ID (8 chars)\", top20[\"node_id\"].str[:8])\n",
        "print(\"\\n[6] Top-20 by appearance within file 6 (diagnostic):\")\n",
        "print(top20[[\"Node ID (8 chars)\",\"appearances_in_file6\",\"% of File6\",\"Cumulative %\"]]\n",
        "      .rename(columns={\"appearances_in_file6\":\"Appearances\"})\n",
        "      .to_string(index=False))\n",
        "\n",
        "# 4) Save CSVs\n",
        "unique_csv = OUTDIR / \"hc_set_unique_ids_from_file6.csv\"\n",
        "counts_csv = OUTDIR / \"hc_set_file6_appearance_counts.csv\"\n",
        "top20_csv  = OUTDIR / \"hc_set_file6_top20_diagnostic.csv\"\n",
        "\n",
        "uniq_ids.to_frame().to_csv(unique_csv, index=False)\n",
        "counts.to_csv(counts_csv, index=False)\n",
        "top20.to_csv(top20_csv, index=False)\n",
        "\n",
        "print(f\"\\n[Saved] Unique IDs: {unique_csv}\")\n",
        "print(f\"[Saved] All appearance counts (diagnostic): {counts_csv}\")\n",
        "print(f\"[Saved] Top-20 (diagnostic): {top20_csv}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kddf90yQLbxG",
        "outputId": "4387eb67-cd6e-431d-c677-fa144f621997"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sniff: /content/drive/MyDrive/DataInBrief-2025/6-High-Capacity-Set.txt ---\n",
            "01: \n",
            "02: High-Capacity\t21:59:00\t\tNode\tID:\tIojVH~dnHmdHpG6ws6-MqFQ9NfIYDIPyT5pYrXH5BS8=\n",
            "03: High-Capacity\t21:59:00\t\tNode\tID:\tKQZPx5UMW8hElPMDGA1CQalD8-K~zV0XIurE5aQLnUI=\n",
            "04: High-Capacity\t21:59:00\t\tNode\tID:\tccMgCyreHWdI55Hj1ehU30cZ5G0d9YVa~6Tr9Iky7j0=\n",
            "05: High-Capacity\t21:59:00\t\tNode\tID:\ttJFdO58cakuNTpGg1uJVL2AJLatabtX8aBHnRCeIiZ4=\n",
            "\n",
            "[6] lines read: 629962\n",
            "[6] extracted IDs: 629934\n",
            "[6] unique node_ids: 2132\n",
            "\n",
            "[6] Top-20 by appearance within file 6 (diagnostic):\n",
            "Node ID (8 chars)  Appearances  % of File6  Cumulative %\n",
            "         Z1bh-f14         3926        0.62          0.62\n",
            "         srLHIzJ8         3799        0.60          1.23\n",
            "         moCfkmqX         3576        0.57          1.79\n",
            "         TDQO~djJ         3369        0.53          2.33\n",
            "         IojVH~dn         3186        0.51          2.83\n",
            "         onjR2Et2         2838        0.45          3.29\n",
            "         tlVPuTPT         2760        0.44          3.72\n",
            "         KM68Y3Pl         2753        0.44          4.16\n",
            "         opSZwy-w         2751        0.44          4.60\n",
            "         Zf30eTRP         2740        0.43          5.03\n",
            "         fgZvKWSc         2511        0.40          5.43\n",
            "         czkf5nIg         2474        0.39          5.82\n",
            "         Pv~HfV8~         2474        0.39          6.22\n",
            "         4RHo4ffX         2446        0.39          6.60\n",
            "         BHIY1YQz         2429        0.39          6.99\n",
            "         RaGUK5Aq         2424        0.38          7.37\n",
            "         YXEAXlOW         2350        0.37          7.75\n",
            "         reNXIMuD         2324        0.37          8.12\n",
            "         IO~OIVIJ         2260        0.36          8.48\n",
            "         wabNC0LP         2235        0.35          8.83\n",
            "\n",
            "[Saved] Unique IDs: /content/drive/MyDrive/DataInBrief-2025/outputs/hc_set_unique_ids_from_file6.csv\n",
            "[Saved] All appearance counts (diagnostic): /content/drive/MyDrive/DataInBrief-2025/outputs/hc_set_file6_appearance_counts.csv\n",
            "[Saved] Top-20 (diagnostic): /content/drive/MyDrive/DataInBrief-2025/outputs/hc_set_file6_top20_diagnostic.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V3Knx4TFMc79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**File 7 - High Capacity and Cummulative Percentages**"
      ],
      "metadata": {
        "id": "RdcyzqCQSKPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Table 2 from file 7 (fallback: file 8), constrained to IDs in file 6\n",
        "import re, os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/DataInBrief-2025\"\n",
        "PATH6 = f\"{BASE}/6-High-Capacity-Set.txt\"\n",
        "PATH7 = f\"{BASE}/7-High-Capacity-Set-Freq.txt\"\n",
        "PATH8 = f\"{BASE}/8-High-Capaity-Freq2.txt\"\n",
        "OUT  = Path(BASE) / \"outputs\"\n",
        "OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RID_RE = re.compile(r\"([A-Za-z0-9\\-\\~\\+\\/]{43}=|[A-Za-z0-9\\-\\~\\+\\/]{44})\")\n",
        "\n",
        "def load_set_ids(path):\n",
        "    ids = []\n",
        "    with open(path, \"r\", errors=\"ignore\") as f:\n",
        "        for ln in f:\n",
        "            m = RID_RE.search(ln)\n",
        "            if m: ids.append(m.group(0).strip())\n",
        "    return pd.Series(ids, name=\"node_id\").drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "def load_freq(path):\n",
        "    # two-column TSV: node_id \\t freq\n",
        "    df = pd.read_csv(path, sep=r\"\\s+\", header=None, names=[\"node_id\",\"freq\"], engine=\"python\")\n",
        "    # keep only sensible rows\n",
        "    df[\"node_id\"] = df[\"node_id\"].astype(str).str.strip()\n",
        "    df[\"freq\"]    = pd.to_numeric(df[\"freq\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"node_id\",\"freq\"])\n",
        "    return df\n",
        "\n",
        "# 1) membership (file 6)\n",
        "set_ids = load_set_ids(PATH6)\n",
        "hc_set = set_ids.to_frame()\n",
        "\n",
        "# 2) frequencies (file 7, else file 8)\n",
        "freq_path = PATH7 if os.path.exists(PATH7) else PATH8\n",
        "df_freq = load_freq(freq_path)\n",
        "\n",
        "# 3) keep only IDs that are in the high-capacity set\n",
        "df_freq = df_freq[df_freq[\"node_id\"].isin(hc_set[\"node_id\"])].copy()\n",
        "\n",
        "# 4) rank + percentages\n",
        "df_freq = df_freq.sort_values(\"freq\", ascending=False).reset_index(drop=True)\n",
        "total = df_freq[\"freq\"].sum()\n",
        "df_freq[\"% of Total\"]   = (100.0 * df_freq[\"freq\"] / total).round(2)\n",
        "df_freq[\"Cumulative %\"] = df_freq[\"% of Total\"].cumsum().round(2)\n",
        "df_freq.insert(0, \"Node ID (8 chars)\", df_freq[\"node_id\"].str[:8])\n",
        "\n",
        "# 5) Top-20 view for Table 2\n",
        "top20 = df_freq.head(20).copy()\n",
        "print(top20[[\"Node ID (8 chars)\", \"freq\", \"% of Total\", \"Cumulative %\"]]\n",
        "      .rename(columns={\"freq\":\"Frequency\"}).to_string(index=False))\n",
        "\n",
        "# Save an export you can paste from\n",
        "top20_out = OUT / \"table2_highcap_top20_with_cumulative.csv\"\n",
        "top20.rename(columns={\"freq\":\"Frequency\"}).to_csv(top20_out, index=False)\n",
        "print(f\"\\nSaved: {top20_out}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghQHowMRSR7f",
        "outputId": "98370b0a-a403-4b3e-a19e-738725b63708"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID (8 chars)  Frequency  % of Total  Cumulative %\n",
            "         tXuneoZN        906        6.99          6.99\n",
            "         TU4AVivT        686        5.29         12.28\n",
            "         x-eBfsgO        414        3.19         15.47\n",
            "         aTdMtnxQ        394        3.04         18.51\n",
            "         ZDz09qEx        270        2.08         20.59\n",
            "         g5xHgSjt        221        1.70         22.29\n",
            "         IiN-TYoM        217        1.67         23.96\n",
            "         CpMHp1xj        176        1.36         25.32\n",
            "         3UavGAjy        172        1.33         26.65\n",
            "         ~Go801IA        168        1.30         27.95\n",
            "         ZB~nvHJj        148        1.14         29.09\n",
            "         SLIdpSD6        144        1.11         30.20\n",
            "         F9577L-s        140        1.08         31.28\n",
            "         bUJ0DwK8        138        1.06         32.34\n",
            "         SC7~pNJt        137        1.06         33.40\n",
            "         z9DJ6Fjl        133        1.03         34.43\n",
            "         RaGUK5Aq        132        1.02         35.45\n",
            "         VoelZCZQ        127        0.98         36.43\n",
            "         9B8cB2oO        125        0.96         37.39\n",
            "         Lid~-zEu        114        0.88         38.27\n",
            "\n",
            "Saved: /content/drive/MyDrive/DataInBrief-2025/outputs/table2_highcap_top20_with_cumulative.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8-ZXsrPcSTat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHeck file 6 for top 20 and check fiel 7 for their frequencies"
      ],
      "metadata": {
        "id": "sNkhR8qoYiCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATE: top-20 from file 6 vs frequencies in file 7\n",
        "import re, os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/DataInBrief-2025\"\n",
        "PATH6 = f\"{BASE}/6-High-Capacity-Set.txt\"\n",
        "PATH7 = f\"{BASE}/7-High-Capacity-Set-Freq.txt\"   # or 8-High-Capaity-Freq2.txt\n",
        "\n",
        "RID_RE = re.compile(r\"([A-Za-z0-9\\-\\~\\+\\/]{43}=|[A-Za-z0-9\\-\\~\\+\\/]{44})\")\n",
        "\n",
        "# --- extract all IDs from file 6 (appearance counts are only diagnostic) ---\n",
        "ids6 = []\n",
        "with open(PATH6, \"r\", errors=\"ignore\") as f:\n",
        "    for ln in f:\n",
        "        m = RID_RE.search(ln)\n",
        "        if m: ids6.append(m.group(0).strip())\n",
        "\n",
        "counts6 = (pd.Series(ids6, name=\"node_id\")\n",
        "             .value_counts()\n",
        "             .rename_axis(\"node_id\")\n",
        "             .reset_index(name=\"appearances_in_file6\"))\n",
        "\n",
        "top20_from6 = counts6.head(20).copy()\n",
        "\n",
        "# --- load file 7 as the authoritative frequency table ---\n",
        "df7 = pd.read_csv(PATH7, sep=r\"\\s+|\\t+\", engine=\"python\",\n",
        "                  header=None, names=[\"node_id\",\"freq7\"])\n",
        "\n",
        "total7 = df7[\"freq7\"].sum()\n",
        "\n",
        "# --- join & report ---\n",
        "j = top20_from6.merge(df7, on=\"node_id\", how=\"left\")\n",
        "j[\"% of Total 7\"] = 100 * j[\"freq7\"] / total7\n",
        "j[\"Cumulative % (7)\"] = j[\"% of Total 7\"].cumsum()\n",
        "j_out = (j.assign(**{\"Node (8)\": j[\"node_id\"].str[:8]})\n",
        "           .loc[:, [\"Node (8)\",\"appearances_in_file6\",\"freq7\",\"% of Total 7\",\"Cumulative % (7)\"]]\n",
        "           .rename(columns={\"appearances_in_file6\":\"Appearances in file6\",\n",
        "                            \"freq7\":\"Frequency (file7)\"})\n",
        "           .round({\"% of Total 7\":2,\"Cumulative % (7)\":2}))\n",
        "\n",
        "print(\"\\nTOP-20 from file 6 mapped to frequencies in file 7:\")\n",
        "print(j_out.sort_values(\"Frequency (file7)\", ascending=False).to_string(index=False))\n",
        "\n",
        "# coverage check\n",
        "missing = j[j[\"freq7\"].isna()]\n",
        "print(f\"\\nCoverage: {len(j)-missing.shape[0]}/20 from file 6 found in file 7.\")\n",
        "if not missing.empty:\n",
        "    print(\"Missing IDs (present in 6, absent in 7):\")\n",
        "    print(\"\\n\".join(missing[\"node_id\"].tolist()))\n",
        "\n",
        "print(f\"\\nTotal selections in file 7: {total7:,}\")\n",
        "print(f\"Top-20 cumulative share (by file 7 frequencies): {j['% of Total 7'].sum():.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3EeaqGqYmUh",
        "outputId": "b533514c-e62f-4bc2-b9d1-d1255d9e4483"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TOP-20 from file 6 mapped to frequencies in file 7:\n",
            "Node (8)  Appearances in file6  Frequency (file7)  % of Total 7  Cumulative % (7)\n",
            "RaGUK5Aq                  2424              132.0          0.59              2.26\n",
            "fgZvKWSc                  2511               84.0          0.38              1.51\n",
            "opSZwy-w                  2751               73.0          0.33              1.10\n",
            "tlVPuTPT                  2760               55.0          0.25              0.74\n",
            "moCfkmqX                  3576               43.0          0.19              0.26\n",
            "YXEAXlOW                  2350               36.0          0.16              2.45\n",
            "TDQO~djJ                  3369               36.0          0.16              0.44\n",
            "reNXIMuD                  2324               31.0          0.14              2.60\n",
            "BHIY1YQz                  2429               22.0          0.10              1.66\n",
            "IO~OIVIJ                  2260               18.0          0.08              2.68\n",
            "Z1bh-f14                  3926               11.0          0.05              0.05\n",
            "4RHo4ffX                  2446                7.0          0.03              1.55\n",
            "Zf30eTRP                  2740                4.0          0.02              1.12\n",
            "moCfkmqX                  3576                3.0          0.01              0.27\n",
            "tlVPuTPT                  2760                3.0          0.01              0.75\n",
            "RaGUK5Aq                  2424                3.0          0.01              2.28\n",
            "TDQO~djJ                  3369                3.0          0.01              0.46\n",
            "TDQO~djJ                  3369                2.0          0.01              0.45\n",
            "tlVPuTPT                  2760                2.0          0.01              0.76\n",
            "BHIY1YQz                  2429                2.0          0.01              1.67\n",
            "onjR2Et2                  2838                1.0          0.00              0.48\n",
            "IojVH~dn                  3186                1.0          0.00              0.48\n",
            "onjR2Et2                  2838                1.0          0.00              0.49\n",
            "onjR2Et2                  2838                1.0          0.00              0.49\n",
            "TDQO~djJ                  3369                1.0          0.00              0.47\n",
            "moCfkmqX                  3576                1.0          0.00              0.28\n",
            "srLHIzJ8                  3799                1.0          0.00              0.06\n",
            "Z1bh-f14                  3926                1.0          0.00              0.06\n",
            "srLHIzJ8                  3799                1.0          0.00              0.07\n",
            "Z1bh-f14                  3926                1.0          0.00              0.05\n",
            "IojVH~dn                  3186                1.0          0.00              0.47\n",
            "TDQO~djJ                  3369                1.0          0.00              0.47\n",
            "opSZwy-w                  2751                1.0          0.00              1.11\n",
            "tlVPuTPT                  2760                1.0          0.00              0.77\n",
            "Pv~HfV8~                  2474                1.0          0.00              1.52\n",
            "czkf5nIg                  2474                1.0          0.00              1.52\n",
            "fgZvKWSc                  2511                1.0          0.00              1.51\n",
            "fgZvKWSc                  2511                1.0          0.00              1.51\n",
            "Zf30eTRP                  2740                1.0          0.00              1.13\n",
            "tlVPuTPT                  2760                1.0          0.00              0.77\n",
            "KM68Y3Pl                  2753                1.0          0.00              0.78\n",
            "4RHo4ffX                  2446                1.0          0.00              1.56\n",
            "RaGUK5Aq                  2424                1.0          0.00              2.29\n",
            "RaGUK5Aq                  2424                1.0          0.00              2.28\n",
            "4RHo4ffX                  2446                1.0          0.00              1.56\n",
            "YXEAXlOW                  2350                1.0          0.00              2.45\n",
            "YXEAXlOW                  2350                1.0          0.00              2.46\n",
            "YXEAXlOW                  2350                1.0          0.00              2.46\n",
            "reNXIMuD                  2324                1.0          0.00              2.60\n",
            "IO~OIVIJ                  2260                1.0          0.00              2.69\n",
            "wabNC0LP                  2235                NaN           NaN               NaN\n",
            "\n",
            "Coverage: 50/20 from file 6 found in file 7.\n",
            "Missing IDs (present in 6, absent in 7):\n",
            "wabNC0LPCKNWLLoctJ4TKkGoeCKuy9LHgkso66a9zWI=\n",
            "\n",
            "Total selections in file 7: 22,317\n",
            "Top-20 cumulative share (by file 7 frequencies): 2.69%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w7epwWT-Ym60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FINAL NEW TABLE 2 - INcludes Cummulative %**"
      ],
      "metadata": {
        "id": "mDjW5YZtcOpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Table 2 (Top-20 High-Capacity nodes) from file 7 with % and Cumulative %\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/DataInBrief-2025\"\n",
        "PATH_FREQ = f\"{BASE}/7-High-Capacity-Set-Freq.txt\"   # authoritative (node_id \\t frequency)\n",
        "OUTDIR = Path(BASE) / \"outputs\"\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load frequency table\n",
        "df = pd.read_csv(PATH_FREQ, sep=r\"\\s+|\\t+\", header=None, names=[\"node_id\",\"frequency\"], engine=\"python\")\n",
        "df[\"node_id\"]   = df[\"node_id\"].astype(str).str.strip()\n",
        "df[\"frequency\"] = pd.to_numeric(df[\"frequency\"], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "# Rank + shares + cumulative\n",
        "df = df.sort_values(\"frequency\", ascending=False, kind=\"mergesort\").reset_index(drop=True)\n",
        "total = df[\"frequency\"].sum()\n",
        "df[\"% of Total\"]   = (100.0 * df[\"frequency\"] / total).round(2)\n",
        "df[\"Cumulative %\"] = df[\"% of Total\"].cumsum().clip(upper=100).round(2)\n",
        "\n",
        "# Top-20 for the manuscript\n",
        "top20 = df.head(20).copy()\n",
        "top20.insert(0, \"Rank\", range(1, len(top20)+1))\n",
        "top20.insert(1, \"Node ID (8 chars)\", top20[\"node_id\"].str[:8])\n",
        "\n",
        "# Pretty print for quick paste into Word\n",
        "print(top20[[\"Rank\",\"Node ID (8 chars)\",\"frequency\",\"% of Total\",\"Cumulative %\"]]\n",
        "      .rename(columns={\"frequency\":\"Frequency\"}).to_string(index=False))\n",
        "\n",
        "# Save CSV for Table 2\n",
        "out_csv = OUTDIR / \"table2_top20_from_file7_with_cumulative.csv\"\n",
        "top20.rename(columns={\"frequency\":\"Frequency\"})[\n",
        "    [\"Rank\",\"node_id\",\"Frequency\",\"% of Total\",\"Cumulative %\"]\n",
        "].to_csv(out_csv, index=False)\n",
        "print(f\"\\nSaved: {out_csv}\")\n",
        "\n",
        "# Text snippet you can paste under the table (shows overall concentration)\n",
        "cum20 = float(top20[\"Cumulative %\"].iloc[-1])\n",
        "print(f\"\\nTop-20 cumulative share of total selections: {cum20:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqwKWk09cTwP",
        "outputId": "1e6f991c-0cc7-49d3-9800-c47245a477ec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Rank Node ID (8 chars)  Frequency  % of Total  Cumulative %\n",
            "    1          tXuneoZN        906        4.06          4.06\n",
            "    2          TU4AVivT        686        3.07          7.13\n",
            "    3          hA~tqWxf        583        2.61          9.74\n",
            "    4          N7T9mVbu        425        1.90         11.64\n",
            "    5          x-eBfsgO        414        1.86         13.50\n",
            "    6          aTdMtnxQ        394        1.77         15.27\n",
            "    7          WwNGsm99        306        1.37         16.64\n",
            "    8          dz3ON-1U        301        1.35         17.99\n",
            "    9          ZDz09qEx        270        1.21         19.20\n",
            "   10          Fu-8GZY8        260        1.17         20.37\n",
            "   11          IwOkup7K        255        1.14         21.51\n",
            "   12          g5xHgSjt        221        0.99         22.50\n",
            "   13          IiN-TYoM        217        0.97         23.47\n",
            "   14          twbWgUGo        196        0.88         24.35\n",
            "   15          CpMHp1xj        176        0.79         25.14\n",
            "   16          3UavGAjy        172        0.77         25.91\n",
            "   17          ~Go801IA        168        0.75         26.66\n",
            "   18          ZB~nvHJj        148        0.66         27.32\n",
            "   19          SLIdpSD6        144        0.65         27.97\n",
            "   20          F9577L-s        140        0.63         28.60\n",
            "\n",
            "Saved: /content/drive/MyDrive/DataInBrief-2025/outputs/table2_top20_from_file7_with_cumulative.csv\n",
            "\n",
            "Top-20 cumulative share of total selections: 28.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UTHWe_qtcX9n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}