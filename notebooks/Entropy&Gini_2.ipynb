{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5H33o8XRSHc",
        "outputId": "733d106b-40ef-4d0b-dd98-92d0e5880b36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "\n",
            "--- Generating Static Table 2 (Top 20 Frequencies from File 7) ---\n",
            "Static Gini (G): 0.2731\n",
            "Static Entropy (H): 4.1467\n",
            "|   Rank | Node ID     |   Selection Frequency |   Cumulative Percentage (%) |\n",
            "|-------:|:------------|----------------------:|----------------------------:|\n",
            "|      1 | TU4AVivT... |                   687 |                       12.2  |\n",
            "|      2 | hA~tqWxf... |                   585 |                       22.58 |\n",
            "|      3 | N7T9mVbu... |                   425 |                       30.13 |\n",
            "|      4 | x-eBfsgO... |                   417 |                       37.53 |\n",
            "|      5 | aTdMtnxQ... |                   395 |                       44.54 |\n",
            "|      6 | WwNGsm99... |                   309 |                       50.03 |\n",
            "|      7 | dz3ON-1U... |                   301 |                       55.37 |\n",
            "|      8 | ZDz09qEx... |                   270 |                       60.16 |\n",
            "|      9 | Fu-8GZY8... |                   260 |                       64.78 |\n",
            "|     10 | IwOkup7K... |                   255 |                       69.31 |\n",
            "|     11 | g5xHgSjt... |                   221 |                       73.23 |\n",
            "|     12 | IiN-TYoM... |                   219 |                       77.12 |\n",
            "|     13 | twbWgUGo... |                   196 |                       80.6  |\n",
            "|     14 | CpMHp1xj... |                   178 |                       83.76 |\n",
            "|     15 | 3UavGAjy... |                   174 |                       86.85 |\n",
            "|     16 | ~Go801IA... |                   168 |                       89.83 |\n",
            "|     17 | ZB~nvHJj... |                   151 |                       92.51 |\n",
            "|     18 | SLIdpSD6... |                   144 |                       95.06 |\n",
            "|     19 | F9577L-s... |                   140 |                       97.55 |\n",
            "|     20 | bUJ0DwK8... |                   138 |                      100    |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2472899224.py:103: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
            "  time_points = pd.date_range(start=df_temporal_merged.index.min(), end=df_temporal_merged.index.max(), freq=freq)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- High-Capacity Temporal Metrics (Time-Series Table) ---\n",
            "Total Windows Computed: 28\n",
            "Average Gini: 0.6758\n",
            "| timestamp           |     gini |   entropy |   share_XfR |\n",
            "|:--------------------|---------:|----------:|------------:|\n",
            "| 2024-11-17 10:00:09 | 0.671497 |   8.68676 |   0.0967742 |\n",
            "| 2024-11-17 10:30:09 | 0.670402 |   8.71162 |   0.09375   |\n",
            "| 2024-11-17 11:00:09 | 0.66527  |   8.74944 |   0.0909091 |\n",
            "| 2024-11-17 11:30:09 | 0.667267 |   8.78537 |   0.0882353 |\n",
            "| 2024-11-17 12:00:09 | 0.668076 |   8.82306 |   0.0909091 |\n",
            "| 2024-11-17 12:30:09 | 0.672325 |   8.79891 |   0.0882353 |\n",
            "| 2024-11-17 13:00:09 | 0.66841  |   8.78667 |   0.0909091 |\n",
            "| 2024-11-17 13:30:09 | 0.669235 |   8.78671 |   0.09375   |\n",
            "| 2024-11-17 14:00:09 | 0.675977 |   8.76115 |   0.0909091 |\n",
            "| 2024-11-17 14:30:09 | 0.668072 |   8.78364 |   0.09375   |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n",
            "/tmp/ipython-input-2472899224.py:106: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
            "  end_time = start_time + pd.Timedelta(window)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import re\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define file paths\n",
        "BASE_PATH = '/content/drive/MyDrive/DataInBrief-2025/'\n",
        "HICAP_FREQ_FILE = BASE_PATH + '7-High-Capacity-Set-Freq.txt'\n",
        "HICAP_TIME_FILE = BASE_PATH + '6-High-Capacity-Set.txt'\n",
        "PROFILES_FILE = BASE_PATH + '10-Profiles-By-Country-anonymized.csv'\n",
        "\n",
        "# --- 2. Metric Functions ---\n",
        "def calculate_entropy(counts):\n",
        "    \"\"\"Calculates Shannon Entropy from a frequency distribution.\"\"\"\n",
        "    c = np.asarray(counts, dtype=float)\n",
        "    c = c[c > 0]\n",
        "    if c.size == 0: return 0.0\n",
        "    p = np.clip(c / c.sum(), 1e-300, 1.0)\n",
        "    return float(-(p * np.log2(p)).sum())\n",
        "\n",
        "def calculate_gini(counts):\n",
        "    \"\"\"Calculates Gini coefficient from a frequency distribution.\"\"\"\n",
        "    c = np.asarray(counts, dtype=float)\n",
        "    c = c[c > 0]\n",
        "    if c.size == 0: return 0.0\n",
        "    s = np.sort(c)\n",
        "    n = s.size\n",
        "    cum = np.cumsum(s)\n",
        "    tot = s.sum()\n",
        "    if tot == 0: return 0.0\n",
        "    return float(1.0 - (2.0 * cum.sum()) / (n * tot) + (1.0 / n))\n",
        "\n",
        "# --- 3. Static Table 2 Calculation (File 7) ---\n",
        "def generate_static_table2(path_freq):\n",
        "    \"\"\"Generates the static top 20 table with Cumulative Percentage.\"\"\"\n",
        "    print(\"\\n--- Generating Static Table 2 (Top 20 Frequencies from File 7) ---\")\n",
        "    df_freq = pd.read_csv(path_freq, sep=r'\\s+', header=None, names=['node_id', 'frequency'], engine='python', usecols=[0, 1], skiprows=1)\n",
        "\n",
        "    # Cleaning and sorting\n",
        "    df_freq[\"node_id\"] = df_freq[\"node_id\"].astype(str).str.strip()\n",
        "    df_freq[\"frequency\"] = pd.to_numeric(df_freq[\"frequency\"], errors=\"coerce\").fillna(0)\n",
        "    df_freq = df_freq.groupby(\"node_id\", as_index=False)[\"frequency\"].sum()\n",
        "    df_top20 = df_freq.sort_values(\"frequency\", ascending=False).head(20).copy()\n",
        "\n",
        "    # Calculations\n",
        "    counts = df_top20['frequency'].values\n",
        "    total_top20_freq = df_top20['frequency'].sum()\n",
        "    df_top20['cumulative_frequency'] = df_top20['frequency'].cumsum()\n",
        "    df_top20['Cumulative Percentage (%)'] = (df_top20['cumulative_frequency'] / total_top20_freq) * 100\n",
        "\n",
        "    # Final Formatting (for display/manuscript)\n",
        "    df_top20['Rank'] = np.arange(1, len(df_top20) + 1)\n",
        "    df_final_table = df_top20[['Rank', 'node_id', 'frequency', 'Cumulative Percentage (%)']].copy()\n",
        "    df_final_table.columns = ['Rank', 'Node ID', 'Selection Frequency', 'Cumulative Percentage (%)']\n",
        "    df_final_table['Cumulative Percentage (%)'] = df_final_table['Cumulative Percentage (%)'].round(2)\n",
        "    df_final_table['Node ID'] = df_final_table['Node ID'].str.slice(0, 8) + '...'\n",
        "\n",
        "    print(f\"Static Gini (G): {calculate_gini(counts):.4f}\")\n",
        "    print(f\"Static Entropy (H): {calculate_entropy(counts):.4f}\")\n",
        "    print(df_final_table.to_markdown(index=False))\n",
        "    return df_final_table\n",
        "\n",
        "# --- 4. Temporal Analysis Core Logic (File 6) ---\n",
        "def load_high_cap_time(path: str) -> pd.DataFrame:\n",
        "    \"\"\"Robust loader for 6-High-Capacity-Set.txt (event log).\"\"\"\n",
        "    TIME_RE = re.compile(r'\\s(\\d{2}:\\d{2}:\\d{2})\\s')\n",
        "    NODE_ID_RE = re.compile(r'Node\\s+ID:\\s+([A-Za-z0-9\\-\\~\\+\\/]{43}=|[A-Za-z0-9\\-\\~\\+\\/]{44})')\n",
        "    ASSUMED_DATE = '2024-11-17'\n",
        "    rows = []\n",
        "    with open(path, \"r\", errors=\"ignore\") as f:\n",
        "        for ln in f:\n",
        "            time_match = TIME_RE.search(ln)\n",
        "            id_match = NODE_ID_RE.search(ln)\n",
        "            if time_match and id_match:\n",
        "                time_str = time_match.group(1)\n",
        "                node_id = id_match.group(1).strip()\n",
        "                try:\n",
        "                    # Parse assuming fixed date derived from other logs\n",
        "                    ts = pd.to_datetime(f\"{ASSUMED_DATE} {time_str}\", errors=\"coerce\")\n",
        "                    if pd.notna(ts):\n",
        "                        rows.append({'timestamp': ts, 'node_id': node_id})\n",
        "                except Exception:\n",
        "                    continue\n",
        "    return pd.DataFrame(rows).sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "\n",
        "def perform_rolling_analysis(df_temporal, df_profiles, window='2H', freq='30T'):\n",
        "    \"\"\"Performs the rolling window analysis and correlation.\"\"\"\n",
        "\n",
        "    # Load and clean profiles for Caps column\n",
        "    df_profiles.columns = [c.strip() for c in df_profiles.columns]\n",
        "    df_profiles = df_profiles.rename(columns={'Full Node ID': 'node_id', 'Caps': 'Caps'})\n",
        "\n",
        "    df_temporal_merged = pd.merge(df_temporal, df_profiles[['node_id', 'Caps']], on='node_id', how='left')\n",
        "    df_temporal_merged.set_index('timestamp', inplace=True)\n",
        "\n",
        "    results = []\n",
        "    time_points = pd.date_range(start=df_temporal_merged.index.min(), end=df_temporal_merged.index.max(), freq=freq)\n",
        "\n",
        "    for start_time in time_points:\n",
        "        end_time = start_time + pd.Timedelta(window)\n",
        "        window_data = df_temporal_merged.loc[start_time:end_time]\n",
        "\n",
        "        if len(window_data) >= 50: # Use 50 as minimum samples for stability\n",
        "            freq_counts = window_data['node_id'].value_counts()\n",
        "            probabilities = freq_counts / freq_counts.sum()\n",
        "            entropy = calculate_entropy(probabilities)\n",
        "            gini = calculate_gini(freq_counts.values)\n",
        "\n",
        "            caps_data = window_data['Caps'].dropna().unique()\n",
        "            # Calculate share of XfR nodes\n",
        "            has_XfR = np.sum(['X' in str(c) and 'f' in str(c) and 'R' in str(c) for c in caps_data])\n",
        "            share_XfR = has_XfR / max(len(caps_data), 1)\n",
        "\n",
        "            results.append({\n",
        "                'timestamp': start_time,\n",
        "                'entropy': entropy,\n",
        "                'gini': gini,\n",
        "                'share_XfR': share_XfR,\n",
        "                'unique_nodes': len(freq_counts)\n",
        "            })\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(\"\\n--- High-Capacity Temporal Metrics (Time-Series Table) ---\")\n",
        "    print(f\"Total Windows Computed: {len(df_results)}\")\n",
        "    print(f\"Average Gini: {df_results['gini'].mean():.4f}\")\n",
        "    print(df_results[['timestamp', 'gini', 'entropy', 'share_XfR']].head(10).to_markdown(index=False))\n",
        "    return df_results\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Load Profiles once\n",
        "        df_profiles = pd.read_csv(PROFILES_FILE)\n",
        "\n",
        "        # 1. Generate Static Table 2\n",
        "        df_table_2 = generate_static_table2(HICAP_FREQ_FILE)\n",
        "\n",
        "        # 2. Generate Time-Series Table\n",
        "        df_high_cap_time = load_high_cap_time(HICAP_TIME_FILE)\n",
        "        df_time_series = perform_rolling_analysis(df_high_cap_time, df_profiles)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"A fatal error occurred during script execution: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SCRIPT 2 - FINAL"
      ],
      "metadata": {
        "id": "7GSYFVDLe23G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import re\n",
        "\n",
        "# --- 0. Configuration and Setup ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "# You should already have this line in your Colab notebook\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Define file paths\n",
        "BASE_PATH = '/content/drive/MyDrive/DataInBrief-2025/'\n",
        "HICAP_FREQ_FILE = BASE_PATH + '7-High-Capacity-Set-Freq.txt'\n",
        "HICAP_TIME_FILE = BASE_PATH + '6-High-Capacity-Set.txt'\n",
        "PROFILES_FILE = BASE_PATH + '10-Profiles-By-Country-anonymized.csv'\n",
        "\n",
        "# --- 1. Metric Functions ---\n",
        "def calculate_entropy(counts):\n",
        "    \"\"\"Calculates Shannon Entropy from a frequency distribution.\"\"\"\n",
        "    c = np.asarray(counts, dtype=float)\n",
        "    c = c[c > 0]\n",
        "    if c.size == 0: return 0.0\n",
        "    p = np.clip(c / c.sum(), 1e-300, 1.0)\n",
        "    return float(-(p * np.log2(p)).sum())\n",
        "\n",
        "def calculate_gini(counts):\n",
        "    \"\"\"Calculates Gini coefficient from a frequency distribution.\"\"\"\n",
        "    c = np.asarray(counts, dtype=float)\n",
        "    c = c[c > 0]\n",
        "    if c.size == 0: return 0.0\n",
        "    s = np.sort(c)\n",
        "    n = s.size\n",
        "    cum = np.cumsum(s)\n",
        "    tot = s.sum()\n",
        "    if tot == 0: return 0.0\n",
        "    return float(1.0 - (2.0 * cum.sum()) / (n * tot) + (1.0 / n))\n",
        "\n",
        "# --- 2. Static Table 2 Calculation (File 7) ---\n",
        "def generate_static_table2(path_freq):\n",
        "    \"\"\"Generates the static top 20 table with Cumulative Percentage.\"\"\"\n",
        "    print(\"\\n--- Generating Static Table 2 (Top 20 Frequencies from File 7) ---\")\n",
        "    df_freq = pd.read_csv(path_freq, sep=r'\\s+', header=None, names=['node_id', 'frequency'], engine='python', usecols=[0, 1], skiprows=1)\n",
        "\n",
        "    # Cleaning and sorting\n",
        "    df_freq[\"node_id\"] = df_freq[\"node_id\"].astype(str).str.strip()\n",
        "    df_freq[\"frequency\"] = pd.to_numeric(df_freq[\"frequency\"], errors=\"coerce\").fillna(0)\n",
        "    df_freq = df_freq.groupby(\"node_id\", as_index=False)[\"frequency\"].sum()\n",
        "    df_top20 = df_freq.sort_values(\"frequency\", ascending=False).head(20).copy()\n",
        "\n",
        "    # Calculations\n",
        "    counts = df_top20['frequency'].values\n",
        "    total_top20_freq = df_top20['frequency'].sum()\n",
        "    df_top20['cumulative_frequency'] = df_top20['frequency'].cumsum()\n",
        "    df_top20['Cumulative Percentage (%)'] = (df_top20['cumulative_frequency'] / total_top20_freq) * 100\n",
        "\n",
        "    # Final Formatting\n",
        "    df_top20['Rank'] = np.arange(1, len(df_top20) + 1)\n",
        "    df_final_table = df_top20[['Rank', 'node_id', 'frequency', 'Cumulative Percentage (%)']].copy()\n",
        "    df_final_table.columns = ['Rank', 'Node ID', 'Selection Frequency', 'Cumulative Percentage (%)']\n",
        "    df_final_table['Cumulative Percentage (%)'] = df_final_table['Cumulative Percentage (%)'].round(2)\n",
        "    df_final_table['Node ID'] = df_final_table['Node ID'].str.slice(0, 8) + '...'\n",
        "\n",
        "    print(f\"Static Gini (G): {calculate_gini(counts):.4f}\")\n",
        "    print(f\"Static Entropy (H): {calculate_entropy(counts):.4f}\")\n",
        "    print(df_final_table.to_markdown(index=False))\n",
        "    return df_final_table\n",
        "\n",
        "# --- 3. Temporal Analysis Core Logic (File 6) ---\n",
        "def load_high_cap_time(path: str) -> pd.DataFrame:\n",
        "    \"\"\"Robust loader for 6-High-Capacity-Set.txt (event log).\"\"\"\n",
        "    TIME_RE = re.compile(r'\\s(\\d{2}:\\d{2}:\\d{2})\\s')\n",
        "    NODE_ID_RE = re.compile(r'Node\\s+ID:\\s+([A-Za-z0-9\\-\\~\\+\\/]{43}=|[A-Za-z0-9\\-\\~\\+\\/]{44})')\n",
        "    ASSUMED_DATE = '2024-11-17'\n",
        "    rows = []\n",
        "    with open(path, \"r\", errors=\"ignore\") as f:\n",
        "        for ln in f:\n",
        "            time_match = TIME_RE.search(ln)\n",
        "            id_match = NODE_ID_RE.search(ln)\n",
        "            if time_match and id_match:\n",
        "                time_str = time_match.group(1)\n",
        "                node_id = id_match.group(1).strip()\n",
        "                try:\n",
        "                    # Parse assuming fixed date derived from other logs\n",
        "                    ts = pd.to_datetime(f\"{ASSUMED_DATE} {time_str}\", errors=\"coerce\")\n",
        "                    if pd.notna(ts):\n",
        "                        rows.append({'timestamp': ts, 'node_id': node_id})\n",
        "                except Exception:\n",
        "                    continue\n",
        "    return pd.DataFrame(rows).sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "\n",
        "def perform_rolling_analysis(df_temporal, df_profiles, window='2h', freq='30min'):\n",
        "    \"\"\"Performs the rolling window analysis and correlation.\"\"\"\n",
        "\n",
        "    # Load and clean profiles for Caps column\n",
        "    df_profiles.columns = [c.strip() for c in df_profiles.columns]\n",
        "    df_profiles = df_profiles.rename(columns={'Full Node ID': 'node_id', 'Caps': 'Caps'})\n",
        "\n",
        "    df_temporal_merged = pd.merge(df_temporal, df_profiles[['node_id', 'Caps']], on='node_id', how='left')\n",
        "    df_temporal_merged.set_index('timestamp', inplace=True)\n",
        "\n",
        "    results = []\n",
        "    time_points = pd.date_range(start=df_temporal_merged.index.min(), end=df_temporal_merged.index.max(), freq=freq)\n",
        "\n",
        "    for start_time in time_points:\n",
        "        end_time = start_time + pd.Timedelta(window)\n",
        "        window_data = df_temporal_merged.loc[start_time:end_time]\n",
        "\n",
        "        if len(window_data) >= 50:\n",
        "            freq_counts = window_data['node_id'].value_counts()\n",
        "            probabilities = freq_counts / freq_counts.sum()\n",
        "            entropy = calculate_entropy(probabilities)\n",
        "            gini = calculate_gini(freq_counts.values)\n",
        "\n",
        "            caps_data = window_data['Caps'].dropna().unique()\n",
        "            # Calculate share of XfR nodes\n",
        "            has_XfR = np.sum(['X' in str(c) and 'f' in str(c) and 'R' in str(c) for c in caps_data])\n",
        "            share_XfR = has_XfR / max(len(caps_data), 1)\n",
        "\n",
        "            results.append({\n",
        "                'timestamp': start_time,\n",
        "                'entropy': entropy,\n",
        "                'gini': gini,\n",
        "                'share_XfR': share_XfR,\n",
        "                'unique_nodes': len(freq_counts)\n",
        "            })\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(\"\\n--- High-Capacity Temporal Metrics (Time-Series Table) ---\")\n",
        "    print(f\"Total Windows Computed: {len(df_results)}\")\n",
        "    print(f\"Average Gini: {df_results['gini'].mean():.4f}\")\n",
        "    print(df_results[['timestamp', 'gini', 'entropy', 'share_XfR']].head(10).to_markdown(index=False))\n",
        "    return df_results\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Load Profiles once\n",
        "        df_profiles = pd.read_csv(PROFILES_FILE)\n",
        "\n",
        "        # 1. Generate Static Table 2\n",
        "        df_table_2 = generate_static_table2(HICAP_FREQ_FILE)\n",
        "\n",
        "        # 2. Generate Time-Series Table\n",
        "        df_high_cap_time = load_high_cap_time(HICAP_TIME_FILE)\n",
        "        df_time_series = perform_rolling_analysis(df_high_cap_time, df_profiles)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"A fatal error occurred during script execution: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm-cHlHIZO2R",
        "outputId": "95f29b07-7c58-4ee5-f3e4-81e668a6d9a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "\n",
            "--- Generating Static Table 2 (Top 20 Frequencies from File 7) ---\n",
            "Static Gini (G): 0.2731\n",
            "Static Entropy (H): 4.1467\n",
            "|   Rank | Node ID     |   Selection Frequency |   Cumulative Percentage (%) |\n",
            "|-------:|:------------|----------------------:|----------------------------:|\n",
            "|      1 | TU4AVivT... |                   687 |                       12.2  |\n",
            "|      2 | hA~tqWxf... |                   585 |                       22.58 |\n",
            "|      3 | N7T9mVbu... |                   425 |                       30.13 |\n",
            "|      4 | x-eBfsgO... |                   417 |                       37.53 |\n",
            "|      5 | aTdMtnxQ... |                   395 |                       44.54 |\n",
            "|      6 | WwNGsm99... |                   309 |                       50.03 |\n",
            "|      7 | dz3ON-1U... |                   301 |                       55.37 |\n",
            "|      8 | ZDz09qEx... |                   270 |                       60.16 |\n",
            "|      9 | Fu-8GZY8... |                   260 |                       64.78 |\n",
            "|     10 | IwOkup7K... |                   255 |                       69.31 |\n",
            "|     11 | g5xHgSjt... |                   221 |                       73.23 |\n",
            "|     12 | IiN-TYoM... |                   219 |                       77.12 |\n",
            "|     13 | twbWgUGo... |                   196 |                       80.6  |\n",
            "|     14 | CpMHp1xj... |                   178 |                       83.76 |\n",
            "|     15 | 3UavGAjy... |                   174 |                       86.85 |\n",
            "|     16 | ~Go801IA... |                   168 |                       89.83 |\n",
            "|     17 | ZB~nvHJj... |                   151 |                       92.51 |\n",
            "|     18 | SLIdpSD6... |                   144 |                       95.06 |\n",
            "|     19 | F9577L-s... |                   140 |                       97.55 |\n",
            "|     20 | bUJ0DwK8... |                   138 |                      100    |\n",
            "\n",
            "--- High-Capacity Temporal Metrics (Time-Series Table) ---\n",
            "Total Windows Computed: 28\n",
            "Average Gini: 0.6758\n",
            "| timestamp           |     gini |   entropy |   share_XfR |\n",
            "|:--------------------|---------:|----------:|------------:|\n",
            "| 2024-11-17 10:00:09 | 0.671497 |   8.68676 |   0.0967742 |\n",
            "| 2024-11-17 10:30:09 | 0.670402 |   8.71162 |   0.09375   |\n",
            "| 2024-11-17 11:00:09 | 0.66527  |   8.74944 |   0.0909091 |\n",
            "| 2024-11-17 11:30:09 | 0.667267 |   8.78537 |   0.0882353 |\n",
            "| 2024-11-17 12:00:09 | 0.668076 |   8.82306 |   0.0909091 |\n",
            "| 2024-11-17 12:30:09 | 0.672325 |   8.79891 |   0.0882353 |\n",
            "| 2024-11-17 13:00:09 | 0.66841  |   8.78667 |   0.0909091 |\n",
            "| 2024-11-17 13:30:09 | 0.669235 |   8.78671 |   0.09375   |\n",
            "| 2024-11-17 14:00:09 | 0.675977 |   8.76115 |   0.0909091 |\n",
            "| 2024-11-17 14:30:09 | 0.668072 |   8.78364 |   0.09375   |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x29hmw32e5V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate New - With Missing %Total"
      ],
      "metadata": {
        "id": "8OUQuUYOXf0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import re\n",
        "\n",
        "# --- 0. Configuration and Setup ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define file paths\n",
        "BASE_PATH = '/content/drive/MyDrive/DataInBrief-2025/'\n",
        "HICAP_FREQ_FILE = BASE_PATH + '7-High-Capacity-Set-Freq.txt'\n",
        "HICAP_TIME_FILE = BASE_PATH + '6-High-Capacity-Set.txt'\n",
        "PROFILES_FILE = BASE_PATH + '10-Profiles-By-Country-anonymized.csv'\n",
        "\n",
        "# --- 1. Metric Functions (Unchanged) ---\n",
        "def calculate_entropy(counts):\n",
        "    \"\"\"Calculates Shannon Entropy from a frequency distribution.\"\"\"\n",
        "    c = np.asarray(counts, dtype=float)\n",
        "    c = c[c > 0]\n",
        "    if c.size == 0: return 0.0\n",
        "    p = np.clip(c / c.sum(), 1e-300, 1.0)\n",
        "    return float(-(p * np.log2(p)).sum())\n",
        "\n",
        "def calculate_gini(counts):\n",
        "    \"\"\"Calculates Gini coefficient from a frequency distribution.\"\"\"\n",
        "    c = np.asarray(counts, dtype=float)\n",
        "    c = c[c > 0]\n",
        "    if c.size == 0: return 0.0\n",
        "    s = np.sort(c)\n",
        "    n = s.size\n",
        "    cum = np.cumsum(s)\n",
        "    tot = s.sum()\n",
        "    if tot == 0: return 0.0\n",
        "    return float(1.0 - (2.0 * cum.sum()) / (n * tot) + (1.0 / n))\n",
        "\n",
        "# --- 2. Static Table 2 Calculation (File 7) - FINAL CORRECTION ---\n",
        "def generate_static_table2(path_freq):\n",
        "    \"\"\"Generates the static top 20 table with Cumulative Percentage and % of Total.\"\"\"\n",
        "    print(\"\\n--- Generating Static Table 2 (Top 20 Frequencies from File 7) ---\")\n",
        "    df_freq = pd.read_csv(path_freq, sep=r'\\s+', header=None, names=['node_id', 'frequency'], engine='python', usecols=[0, 1], skiprows=1)\n",
        "\n",
        "    # Cleaning and sorting\n",
        "    df_freq[\"node_id\"] = df_freq[\"node_id\"].astype(str).str.strip()\n",
        "    df_freq[\"frequency\"] = pd.to_numeric(df_freq[\"frequency\"], errors=\"coerce\").fillna(0)\n",
        "    df_freq = df_freq.groupby(\"node_id\", as_index=False)[\"frequency\"].sum()\n",
        "    df_top20 = df_freq.sort_values(\"frequency\", ascending=False).head(20).copy()\n",
        "\n",
        "    # Calculations\n",
        "    counts = df_top20['frequency'].values\n",
        "    total_top20_freq = df_top20['frequency'].sum()\n",
        "\n",
        "    # CRITICAL: Calculate individual percentage of the top 20 subset\n",
        "    df_top20['% of Total (Node)'] = (df_top20['frequency'] / total_top20_freq) * 100\n",
        "\n",
        "    # Calculate Cumulative Percentage\n",
        "    df_top20['cumulative_frequency'] = df_top20['frequency'].cumsum()\n",
        "    df_top20['Cumulative Percentage (%)'] = (df_top20['cumulative_frequency'] / total_top20_freq) * 100\n",
        "\n",
        "    # Final Formatting\n",
        "    df_top20['Rank'] = np.arange(1, len(df_top20) + 1)\n",
        "    df_final_table = df_top20[['Rank', 'node_id', 'frequency', '% of Total (Node)', 'Cumulative Percentage (%)']].copy()\n",
        "\n",
        "    # Rename columns for manuscript\n",
        "    df_final_table.columns = ['Rank', 'Node ID', 'Selection Frequency', '% of Total', 'Cumulative Percentage (%)']\n",
        "\n",
        "    # Rounding and Node ID formatting\n",
        "    df_final_table['% of Total'] = df_final_table['% of Total'].round(2)\n",
        "    df_final_table['Cumulative Percentage (%)'] = df_final_table['Cumulative Percentage (%)'].round(2)\n",
        "    df_final_table['Node ID'] = df_final_table['Node ID'].str.slice(0, 8) + '...'\n",
        "\n",
        "    print(f\"Static Gini (G): {calculate_gini(counts):.4f}\")\n",
        "    print(f\"Static Entropy (H): {calculate_entropy(counts):.4f}\")\n",
        "    print(df_final_table.to_markdown(index=False))\n",
        "    return df_final_table\n",
        "\n",
        "# --- 3. Temporal Analysis Core Logic (File 6) ---\n",
        "def load_high_cap_time(path: str) -> pd.DataFrame:\n",
        "    \"\"\"Robust loader for 6-High-Capacity-Set.txt (event log).\"\"\"\n",
        "    TIME_RE = re.compile(r'\\s(\\d{2}:\\d{2}:\\d{2})\\s')\n",
        "    NODE_ID_RE = re.compile(r'Node\\s+ID:\\s+([A-Za-z0-9\\-\\~\\+\\/]{43}=|[A-Za-z0-9\\-\\~\\+\\/]{44})')\n",
        "    ASSUMED_DATE = '2024-11-17'\n",
        "    rows = []\n",
        "    with open(path, \"r\", errors=\"ignore\") as f:\n",
        "        for ln in f:\n",
        "            time_match = TIME_RE.search(ln)\n",
        "            id_match = NODE_ID_RE.search(ln)\n",
        "            if time_match and id_match:\n",
        "                time_str = time_match.group(1)\n",
        "                node_id = id_match.group(1).strip()\n",
        "                try:\n",
        "                    # Parse assuming fixed date derived from other logs\n",
        "                    ts = pd.to_datetime(f\"{ASSUMED_DATE} {time_str}\", errors=\"coerce\")\n",
        "                    if pd.notna(ts):\n",
        "                        rows.append({'timestamp': ts, 'node_id': node_id})\n",
        "                except Exception:\n",
        "                    continue\n",
        "    return pd.DataFrame(rows).sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "\n",
        "def perform_rolling_analysis(df_temporal, df_profiles, window='2h', freq='30min'):\n",
        "    \"\"\"Performs the rolling window analysis and correlation.\"\"\"\n",
        "\n",
        "    # Load and clean profiles for Caps column\n",
        "    df_profiles.columns = [c.strip() for c in df_profiles.columns]\n",
        "    df_profiles = df_profiles.rename(columns={'Full Node ID': 'node_id', 'Caps': 'Caps'})\n",
        "\n",
        "    df_temporal_merged = pd.merge(df_temporal, df_profiles[['node_id', 'Caps']], on='node_id', how='left')\n",
        "    df_temporal_merged.set_index('timestamp', inplace=True)\n",
        "\n",
        "    results = []\n",
        "    time_points = pd.date_range(start=df_temporal_merged.index.min(), end=df_temporal_merged.index.max(), freq=freq)\n",
        "\n",
        "    for start_time in time_points:\n",
        "        end_time = start_time + pd.Timedelta(window)\n",
        "        window_data = df_temporal_merged.loc[start_time:end_time]\n",
        "\n",
        "        if len(window_data) >= 50:\n",
        "            freq_counts = window_data['node_id'].value_counts()\n",
        "            probabilities = freq_counts / freq_counts.sum()\n",
        "            entropy = calculate_entropy(probabilities)\n",
        "            gini = calculate_gini(freq_counts.values)\n",
        "\n",
        "            caps_data = window_data['Caps'].dropna().unique()\n",
        "            # Calculate share of XfR nodes\n",
        "            has_XfR = np.sum(['X' in str(c) and 'f' in str(c) and 'R' in str(c) for c in caps_data])\n",
        "            share_XfR = has_XfR / max(len(caps_data), 1)\n",
        "\n",
        "            results.append({\n",
        "                'timestamp': start_time,\n",
        "                'entropy': entropy,\n",
        "                'gini': gini,\n",
        "                'share_XfR': share_XfR,\n",
        "                'unique_nodes': len(freq_counts)\n",
        "            })\n",
        "\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(\"\\n--- High-Capacity Temporal Metrics (Time-Series Table) ---\")\n",
        "    print(f\"Total Windows Computed: {len(df_results)}\")\n",
        "    print(f\"Average Gini: {df_results['gini'].mean():.4f}\")\n",
        "    print(df_results[['timestamp', 'gini', 'entropy', 'share_XfR']].head(10).to_markdown(index=False))\n",
        "    return df_results\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Load Profiles once\n",
        "        df_profiles = pd.read_csv(PROFILES_FILE)\n",
        "\n",
        "        # 1. Generate Static Table 2\n",
        "        df_table_2 = generate_static_table2(HICAP_FREQ_FILE)\n",
        "\n",
        "        # 2. Generate Time-Series Table\n",
        "        df_high_cap_time = load_high_cap_time(HICAP_TIME_FILE)\n",
        "        df_time_series = perform_rolling_analysis(df_high_cap_time, df_profiles)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"A fatal error occurred during script execution: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b0ObINKXkbb",
        "outputId": "0c098f48-6918-4e6d-f62a-1005b0dbd79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "\n",
            "--- Generating Static Table 2 (Top 20 Frequencies from File 7) ---\n",
            "Static Gini (G): 0.2731\n",
            "Static Entropy (H): 4.1467\n",
            "|   Rank | Node ID     |   Selection Frequency |   % of Total |   Cumulative Percentage (%) |\n",
            "|-------:|:------------|----------------------:|-------------:|----------------------------:|\n",
            "|      1 | TU4AVivT... |                   687 |        12.2  |                       12.2  |\n",
            "|      2 | hA~tqWxf... |                   585 |        10.39 |                       22.58 |\n",
            "|      3 | N7T9mVbu... |                   425 |         7.54 |                       30.13 |\n",
            "|      4 | x-eBfsgO... |                   417 |         7.4  |                       37.53 |\n",
            "|      5 | aTdMtnxQ... |                   395 |         7.01 |                       44.54 |\n",
            "|      6 | WwNGsm99... |                   309 |         5.49 |                       50.03 |\n",
            "|      7 | dz3ON-1U... |                   301 |         5.34 |                       55.37 |\n",
            "|      8 | ZDz09qEx... |                   270 |         4.79 |                       60.16 |\n",
            "|      9 | Fu-8GZY8... |                   260 |         4.62 |                       64.78 |\n",
            "|     10 | IwOkup7K... |                   255 |         4.53 |                       69.31 |\n",
            "|     11 | g5xHgSjt... |                   221 |         3.92 |                       73.23 |\n",
            "|     12 | IiN-TYoM... |                   219 |         3.89 |                       77.12 |\n",
            "|     13 | twbWgUGo... |                   196 |         3.48 |                       80.6  |\n",
            "|     14 | CpMHp1xj... |                   178 |         3.16 |                       83.76 |\n",
            "|     15 | 3UavGAjy... |                   174 |         3.09 |                       86.85 |\n",
            "|     16 | ~Go801IA... |                   168 |         2.98 |                       89.83 |\n",
            "|     17 | ZB~nvHJj... |                   151 |         2.68 |                       92.51 |\n",
            "|     18 | SLIdpSD6... |                   144 |         2.56 |                       95.06 |\n",
            "|     19 | F9577L-s... |                   140 |         2.49 |                       97.55 |\n",
            "|     20 | bUJ0DwK8... |                   138 |         2.45 |                      100    |\n",
            "\n",
            "--- High-Capacity Temporal Metrics (Time-Series Table) ---\n",
            "Total Windows Computed: 28\n",
            "Average Gini: 0.6758\n",
            "| timestamp           |     gini |   entropy |   share_XfR |\n",
            "|:--------------------|---------:|----------:|------------:|\n",
            "| 2024-11-17 10:00:09 | 0.671497 |   8.68676 |   0.0967742 |\n",
            "| 2024-11-17 10:30:09 | 0.670402 |   8.71162 |   0.09375   |\n",
            "| 2024-11-17 11:00:09 | 0.66527  |   8.74944 |   0.0909091 |\n",
            "| 2024-11-17 11:30:09 | 0.667267 |   8.78537 |   0.0882353 |\n",
            "| 2024-11-17 12:00:09 | 0.668076 |   8.82306 |   0.0909091 |\n",
            "| 2024-11-17 12:30:09 | 0.672325 |   8.79891 |   0.0882353 |\n",
            "| 2024-11-17 13:00:09 | 0.66841  |   8.78667 |   0.0909091 |\n",
            "| 2024-11-17 13:30:09 | 0.669235 |   8.78671 |   0.09375   |\n",
            "| 2024-11-17 14:00:09 | 0.675977 |   8.76115 |   0.0909091 |\n",
            "| 2024-11-17 14:30:09 | 0.668072 |   8.78364 |   0.09375   |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8P084uzzXlLs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}